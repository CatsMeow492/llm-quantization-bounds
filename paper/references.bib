@article{hu2021lora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2022}
}

@article{dettmers2023qlora,
  title={QLoRA: Efficient Finetuning of Quantized LLMs},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2305.14314},
  year={2023}
}

@article{xu2023qr-adaptor,
  title={QR-Adaptor: Efficient Low-Rank Adaptation with Quantization for Large Language Models},
  author={Xu, Yichen and Zhang, Qingru and Wang, Bingxuan and Lee, Yue},
  journal={arXiv preprint arXiv:2309.02233},
  year={2023}
}

@article{jacob2018quantization,
  title={Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  journal={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2704--2713},
  year={2018}
}

@article{banner2018post,
  title={Post training 4-bit quantization of convolutional networks for rapid-deployment},
  author={Banner, Ron and Nahshan, Yury and Hoffer, Elad and Soudry, Daniel},
  journal={arXiv preprint arXiv:1810.05723},
  year={2018}
}

@article{esser2019learned,
  title={Learned step size quantization},
  author={Esser, Steven K and McKinstry, Jeffrey L and Bablani, Deepika and Appuswamy, Rathinakumar and Modha, Dharmendra S},
  journal={arXiv preprint arXiv:1902.08153},
  year={2019}
}

@article{nagel2020up,
  title={Up or down? adaptive rounding for post-training quantization},
  author={Nagel, Markus and Amjad, Rana Ali and Van Baalen, Mart and Louizos, Christos and Blankevoort, Tijmen},
  journal={International Conference on Machine Learning},
  pages={7197--7206},
  year={2020}
}

@article{malladi2023kernel,
  title={Kernel and Rich Regimes in Overparametrized Models},
  author={Malladi, Sadhika and Gao, Tianyu and Nichani, Eshaan and Damian, Alex and Lee, Jason D and Chen, Danqi and Arora, Sanjeev},
  journal={arXiv preprint arXiv:2402.02394},
  year={2023}
}

@article{wang2023lora,
  title={LoRA Training Does Not Suffer From Catastrophic Forgetting},
  author={Wang, Liyuan and Yu, Jingbo and Huang, Xingcheng and Rong, Suqi and Xiong, Chenxi and Zhu, Hao},
  journal={arXiv preprint arXiv:2405.09673},
  year={2023}
}

@article{chen2020understanding,
  title={Understanding gradient clipping in private SGD: A geometric perspective},
  author={Chen, Xiangyi and Kamath, Gautam and Kulkarni, Janardhan and Wu, Zhiwei Steven},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={13773--13782},
  year={2020}
}

@article{zhang2019dive,
  title={A dive into understanding gradient flow in low-rank matrix factorization},
  author={Zhang, Xiao and Wainwright, Martin J},
  journal={arXiv preprint arXiv:1905.03590},
  year={2019}
}

@article{kratsios2021approximation,
  title={Approximation capabilities of neural ODEs and invertible residual networks},
  author={Kratsios, Anastasis and Papon, L{\'e}on},
  journal={International Conference on Machine Learning},
  pages={5811--5820},
  year={2021}
}

@article{li2018visualizing,
  title={Visualizing the loss landscape of neural nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  journal={arXiv preprint arXiv:1712.09913},
  year={2018}
}

@article{fort2019deep,
  title={Deep ensembles: A loss landscape perspective},
  author={Fort, Stanislav and Ganguli, Surya},
  journal={arXiv preprint arXiv:1912.02757},
  year={2019}
}

@article{zhang2017understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1611.03530},
  year={2017}
}

@article{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  journal={arXiv preprint arXiv:1706.08947},
  year={2017}
}

@article{bartlett2017spectrally,
  title={Spectrally-normalized margin bounds for neural networks},
  author={Bartlett, Peter L and Foster, Dylan J and Telgarsky, Matus J},
  journal={arXiv preprint arXiv:1706.08498},
  year={2017}
}

@article{golowich2018size,
  title={Size-independent sample complexity of neural networks},
  author={Golowich, Noah and Rakhlin, Alexander and Shamir, Ohad},
  journal={arXiv preprint arXiv:1712.06541},
  year={2018}
}

@article{wei2019regularization,
  title={Regularization matters: Generalization and optimization of neural nets vs their induced kernel},
  author={Wei, Colin and Lee, Jason D and Liu, Qiang and Ma, Tengyu},
  journal={arXiv preprint arXiv:1810.05369},
  year={2019}
}

@article{arora2019fine,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Ruslan R and Wang, Ruosong},
  journal={International Conference on Machine Learning},
  pages={322--332},
  year={2019}
} 

@article{liu2022quantnoise,
  title={Quantization Can Be Equivalent to Adding "Gradient Noise"},
  author={Liu, Zhuang and Liu, Huan},
  journal={arXiv preprint arXiv:2205.14235},
  year={2022}
} 